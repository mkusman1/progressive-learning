{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from math import log2, ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.deciders import SimpleArgmaxAverage\n",
    "from proglearn.transformers import TreeClassificationTransformer, NeuralClassificationTransformer\n",
    "from proglearn.voters import TreeClassificationVoter, KNNClassificationVoter\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def LF_experiment(train_x, train_y, test_x, test_y, ntrees, shift, slot, model, num_points_per_task, acorn=None):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    shifts = []\n",
    "    tasks = []\n",
    "    base_tasks = []\n",
    "    accuracies_across_tasks = []\n",
    "    train_times_across_tasks = []\n",
    "    inference_times_across_tasks = []\n",
    "\n",
    "    if model == \"dnn\":\n",
    "        default_transformer_class = NeuralClassificationTransformer\n",
    "\n",
    "        network = keras.Sequential()\n",
    "        network.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=np.shape(train_x)[1:]))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=254, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "\n",
    "        network.add(layers.Flatten())\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(2000, activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(2000, activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(units=10, activation = 'softmax'))\n",
    "\n",
    "        default_transformer_kwargs = {\"network\" : network,\n",
    "                                      \"euclidean_layer_idx\" : -2,\n",
    "                                      \"num_classes\" : 10,\n",
    "                                      \"optimizer\" : keras.optimizers.Adam(3e-4)\n",
    "                                     }\n",
    "\n",
    "        default_voter_class = KNNClassificationVoter\n",
    "        default_voter_kwargs = {\"k\" : 16 * int(np.log2(num_points_per_task))}\n",
    "\n",
    "        default_decider_class = SimpleArgmaxAverage\n",
    "    elif model == \"uf\":\n",
    "        default_transformer_class = TreeClassificationTransformer\n",
    "        default_transformer_kwargs = {\"kwargs\" : {\"max_depth\" : 30}}\n",
    "\n",
    "        default_voter_class = TreeClassificationVoter\n",
    "        default_voter_kwargs = {}\n",
    "\n",
    "        default_decider_class = SimpleArgmaxAverage\n",
    "    progressive_learner = ProgressiveLearner(default_transformer_class = default_transformer_class,\n",
    "                                         default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                         default_voter_class = default_voter_class,\n",
    "                                         default_voter_kwargs = default_voter_kwargs,\n",
    "                                         default_decider_class = default_decider_class)\n",
    "\n",
    "    for task_ii in range(10):\n",
    "        print(\"Starting Task {} For Fold {} For Slot {}\".format(task_ii, shift, slot))\n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "\n",
    "        train_start_time = time.time()\n",
    "        progressive_learner.add_task(\n",
    "            X = train_x[task_ii*5000+slot*num_points_per_task:task_ii*5000+(slot+1)*num_points_per_task],\n",
    "            y = train_y[task_ii*5000+slot*num_points_per_task:task_ii*5000+(slot+1)*num_points_per_task],\n",
    "            num_transformers = 1 if model == \"dnn\" else ntrees,\n",
    "            transformer_voter_decider_split = [0.67, 0.33, 0],\n",
    "            decider_kwargs = {\"classes\" : np.unique(train_y[task_ii*5000+slot*num_points_per_task:task_ii*5000+(slot+1)*num_points_per_task])},\n",
    "            backward_task_ids=[0]\n",
    "            )\n",
    "        train_end_time = time.time()\n",
    "\n",
    "        inference_start_time = time.time()\n",
    "        llf_task=progressive_learner.predict(\n",
    "            test_x[:1000], task_id=0\n",
    "            )\n",
    "        inference_end_time = time.time()\n",
    "        acc = np.mean(\n",
    "                    llf_task == test_y[:1000]\n",
    "                    )\n",
    "        accuracies_across_tasks.append(acc)\n",
    "        shifts.append(shift)\n",
    "        train_times_across_tasks.append(train_end_time - train_start_time)\n",
    "        inference_times_across_tasks.append(inference_end_time - inference_start_time)\n",
    "\n",
    "        print(\"Accuracy Across Tasks: {}\".format(accuracies_across_tasks))\n",
    "        print(\"Train Times Across Tasks: {}\".format(train_times_across_tasks))\n",
    "        print(\"Inference Times Across Tasks: {}\".format(inference_times_across_tasks))\n",
    "\n",
    "    df['data_fold'] = shifts\n",
    "    df['task'] = range(1, 11)\n",
    "    df['task_1_accuracy'] = accuracies_across_tasks\n",
    "    df['train_times'] = train_times_across_tasks\n",
    "    df['inference_times'] = inference_times_across_tasks\n",
    "\n",
    "    file_to_save = 'result/'+model+str(ntrees)+'_'+str(shift)+'_'+str(slot)+'.pickle'\n",
    "    with open(file_to_save, 'wb') as f:\n",
    "        pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def cross_val_data(data_x, data_y, num_points_per_task, total_task=10, shift=1):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = [np.where(data_y == u)[0] for u in np.unique(data_y)]\n",
    "\n",
    "    batch_per_task=5000//num_points_per_task\n",
    "    sample_per_class = num_points_per_task//total_task\n",
    "\n",
    "    for task in range(total_task):\n",
    "        for batch in range(batch_per_task):\n",
    "            for class_no in range(task*10,(task+1)*10,1):\n",
    "                indx = np.roll(idx[class_no],(shift-1)*100)\n",
    "\n",
    "                if batch==0 and class_no==0 and task==0:\n",
    "                    train_x = x[indx[batch*sample_per_class:(batch+1)*sample_per_class]]\n",
    "                    test_x = x[indx[batch*total_task+num_points_per_task:(batch+1)*total_task+num_points_per_task]]\n",
    "                    train_y = np.random.randint(low = 0, high = total_task, size = sample_per_class)\n",
    "                    test_y = np.random.randint(low = 0, high = total_task, size = total_task)\n",
    "                else:\n",
    "                    train_x = np.concatenate((train_x, x[indx[batch*sample_per_class:(batch+1)*sample_per_class]]), axis=0)\n",
    "                    test_x = np.concatenate((test_x, x[indx[batch*total_task+num_points_per_task:(batch+1)*total_task+num_points_per_task]]), axis=0)\n",
    "                    if task == 0:\n",
    "                        train_y = np.concatenate((train_y, y[indx[batch*sample_per_class:(batch+1)*sample_per_class]]), axis=0)\n",
    "                        test_y = np.concatenate((test_y, y[indx[batch*total_task+num_points_per_task:(batch+1)*total_task+num_points_per_task]]), axis=0)\n",
    "                    else:\n",
    "                        train_y = np.concatenate((train_y, np.random.randint(low = 0, high = total_task, size = sample_per_class)), axis=0)\n",
    "                        test_y = np.concatenate((test_y, np.random.randint(low = 0, high = total_task, size = total_task)), axis = 0)\n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def run_parallel_exp(data_x, data_y, n_trees, model, num_points_per_task, slot=0, shift=1):\n",
    "    train_x, train_y, test_x, test_y = cross_val_data(data_x, data_y, num_points_per_task, shift=shift)\n",
    "\n",
    "    if model == \"dnn\":\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config=config)\n",
    "        with tf.device('/gpu:'+str(shift % 4)):\n",
    "            LF_experiment(train_x, train_y, test_x, test_y, n_trees, shift, slot, model, num_points_per_task, acorn=12345)\n",
    "    else:\n",
    "        LF_experiment(train_x, train_y, test_x, test_y, n_trees, shift, slot, model, num_points_per_task, acorn=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "### MAIN HYPERPARAMS ###\n",
    "model = \"uf\"\n",
    "num_points_per_task = 500\n",
    "########################\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "data_x = np.concatenate([X_train, X_test])\n",
    "if model == \"uf\":\n",
    "    data_x = data_x.reshape((data_x.shape[0], data_x.shape[1] * data_x.shape[2] * data_x.shape[3]))\n",
    "data_y = np.concatenate([y_train, y_test])\n",
    "data_y = data_y[:, 0]\n",
    "\n",
    "slot_fold = range(int(5000 // num_points_per_task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "if model == \"uf\":\n",
    "    shift_fold = range(1,7,1)\n",
    "    n_trees=[10]\n",
    "    iterable = product(n_trees,shift_fold,slot_fold)\n",
    "    Parallel(n_jobs=-2,verbose=1)(\n",
    "        delayed(run_parallel_exp)(\n",
    "                data_x, data_y, ntree, model, num_points_per_task, slot=slot, shift=shift\n",
    "                ) for ntree,shift,slot in iterable\n",
    "                )\n",
    "elif model == \"dnn\":\n",
    "\n",
    "    for slot in slot_fold:\n",
    "        def perform_shift(shift):\n",
    "            return run_parallel_exp(data_x, data_y, 0, model, num_points_per_task, slot=slot, shift=shift)\n",
    "\n",
    "        stage_1_shifts = range(1, 5)\n",
    "        with Pool(4) as p:\n",
    "            p.map(perform_shift, stage_1_shifts)\n",
    "\n",
    "        stage_2_shifts = range(5, 7)\n",
    "        with Pool(4) as p:\n",
    "            p.map(perform_shift, stage_2_shifts)\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
